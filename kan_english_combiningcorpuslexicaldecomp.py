# -*- coding: utf-8 -*-
"""Kan_English_CombiningCorpusLexicalDecomp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UOYC37fR1Z161U2hKU-6fYTsNfZmtEJq
"""

import re #for data formatting
import numpy as np #for data storage
import pandas as pd
import helper as hp
from nltk.corpus import  wordnet as wn

alpha = 0.004 #0.2
beta  = 0.15 #0.45
benchmark_similarity = 0.8025
gamma = 1.8
#score_factor = 0.35

#!pip install pywsd

!pip install -U wn==0.0.22

!pip install nltk

import nltk
#from pywsd import disambiguate
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
#from nltk.corpus import wordnet as wn

from nltk.tag.perceptron import PerceptronTagger
def tagSentence(text):
  pos_sent = nltk.pos_tag(text)
  return pos_sent

#Run this first to acess spacy.load('en_core_web_lg')
#!python -m spacy download en_core_web_lg

import spacy #for nlp based operations
from scipy import spatial #used for cosine similarity

#nlp = spacy.load('en_core_web_sm')
nlp = spacy.load('en_core_web_lg')

from google.colab import drive
drive.mount('/content/drive')

def getTokens(sent):
  doc = nlp(sent)
  sent_tokenized=[]
  for token in doc:
    if(token.is_stop!=True):
      #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, )
      sent_tokenized.append(token.lemma_)
  return sent_tokenized

sent1= "ಮಿಂಚುಹುಳು ಲ್ಯಾಂಪಿರಿಡೆ ಎಂಬ ಕೀಟಗಳ, ಕೊಲಿಯೋಪ್ಟೆರ ಕುಟುಂಬಕ್ಕೆ ಸೇರುತ್ತದೆ." #"Red alcoholic drink" #"A group of kids is playing in a yard and an old man is standing in the background"
sent2="The horseshoe crab is an arthropod of the family Limulidae." #"A bottle of wine"#A group of boys in a yard is playing and a man is standing in the background"

sent1_tokenized = getTokens(sent1)
sent2_tokenized = getTokens(sent2)
M = {s : {t:0 for t in sent2_tokenized}  for s in sent1_tokenized }
print(M)

def preprocess_sentence ( sent ) :
    """
        1. Disambiguate ; 2. Replace words from the custom dictionary, if word not present in wordnet , currently dictionary empty so not used
        ( No checking with Meddra, since if present in Meddra, then the word will be taken care of, in Medical similarity algorithm
        also, if present in Meddra, then the word won't be present in the custom dictionary )
        3. Remove stop words ; 4. Change tags to Medical, if a Medical term found  ; 5. Remove the unimportant ones
    """
    sent_wsd = disambiguate(sent) ## disambiguate with maximum information available
    sent_wsd = hp.remove_stop_words( sent_wsd ) ## remove stop words afer that
    return  (' ').join([w[0] for w in sent_wsd]), sent_wsd ## sentence in string format , sentence broken down and returned

def wordnet_synset_similarity(s1,s2, tagged_s1, tagged_s2, dict_mat):
  #s1_filtered = filterWordsforWordnet(s1_tagged)

   # L1 =dict()
   # L2 =defaultdict(list)              #never raises a key error.  takes no arguments and provides the default value for a nonexistent key.
    for i, syn1 in enumerate(s1):
        #L1[syn1[0]] = list()              #first element of s1_wsd
        for j, syn2 in enumerate(s2):
            lemma1 = syn1.lemma_names()[0]
            lemma2 = syn2.lemma_names()[0]
            if  lemma1 == lemma2  : ## thus even for non- identified terms if the other sentences have the sentence, scores get pushed up
              dict_mat[tagged_s1[i]][tagged_s2[j]]=0.99
              continue
            s1_ss = set(wn.synsets(tagged_s1[i]))
            s2_ss = set(wn.synsets(tagged_s2[j]))
            #print(s1_ss.intersection(s2_ss))
            if (s1_ss.intersection(s2_ss)):
              dict_mat[tagged_s1[i]][tagged_s2[j]]=0.99
            elif syn1.pos() and syn2.pos() : ## tags for both the queries aren't None
                subsumer = syn1.lowest_common_hypernyms(syn2, simulate_root=True)[0]   #find the subsumer for two given words that are taken into consideration
                h =subsumer.max_depth() + 1         # finding their depth

                syn1_dist_subsumer = syn1.shortest_path_distance(subsumer,simulate_root =True)        #finding the path length
                syn2_dist_subsumer = syn2.shortest_path_distance(subsumer,simulate_root =True)       #finding the path length

                l  = syn1_dist_subsumer + syn2_dist_subsumer
                f1 = np.exp(-alpha*l)
                a  = np.exp(beta*h)
                b  = np.exp(-beta*h)                   #necessary calculations to arrive at a similarity score
                f2 = (a-b) /(a+b)
                sim = f1*f2
                dict_mat[tagged_s1[i]][tagged_s2[j]]=sim
            else :
                dict_mat[tagged_s1[i]][tagged_s2[j]]= 0
    return dict_mat

from nltk.tokenize import TreebankWordTokenizer
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('omw-1.4')


# def tokenizeSentence(Sentence):
#   tokenizer = TreebankWordTokenizer()
#   text = tokenizer.tokenize(Sentence)
#   return text

def removeStopwords(Text):
  cleantext = [word for word in Text if not word in stopwords.words('english')]
  return cleantext

#print('man' in stopwords.words('english'))

from nltk.tag.perceptron import PerceptronTagger
def tagSentence(text):
  pos_sent = nltk.pos_tag(text)
  return pos_sent

from nltk.corpus import wordnet
def filterWordsforWordnet(pos_sent):
  pos_sent_wordnet = []
  for tokens in pos_sent:
    #check the part of speech. If Noun, Verb, Adverb or Adjective then get the synset
    if tokens[1] in ('NN', 'NNS', 'NNP','NNPS', 'VB', 'VBZ', 'VBD', 'VBG', 'VBP', 'VBN', 'RB', 'RBR', 'RBS', 'JJ','JJR','JJS'):
      pos_sent_wordnet.append(tokens[0])
  return pos_sent_wordnet

from nltk.wsd import lesk
def disambiguate(pos_sent_wordnet):
  wsdSentence= []
  for i in pos_sent_wordnet:
    wsdSentence.append(lesk(pos_sent_wordnet,i))
  return wsdSentence

# S1= "A jewel is a precious stone used to decorate valuable things that you wear, such as rings or necklaces."
# S2= "A gem is a jewel or stone that is used in jewellery."

!pip install fasttext

import fasttext
import fasttext.util
from google.colab import drive
#drive.mount('/content/drive')
ft = fasttext.lo('./drive/MyDrive/Colab Notebooks/KN-ENG-Samanantar/wiki.kn.bin')

#1. Sentence Tokenize
s1_tokenized = getTokens(sent1)
#Remove Stopwords
s1_clean_tokenized = removeStopwords(s1_tokenized)
#Perform Tagging
s1_tagged = tagSentence(s1_clean_tokenized)
s1_filtered = filterWordsforWordnet(s1_tagged)
#Disambiguate the word using pyWSD
s1_disambiguated = disambiguate(s1_clean_tokenized)

s2_tokenized = getTokens(sent2)
s2_clean_tokenized = removeStopwords(s2_tokenized)
s2_tagged = tagSentence(s2_clean_tokenized)
s2_filtered = filterWordsforWordnet(s2_tagged)
s2_disambiguated = disambiguate(s2_clean_tokenized)

print(s1_disambiguated)
print(s2_disambiguated)

# from collections import defaultdict

# print(s1_tagged)
# for syn1 in s1_disambiguated:
#   print(syn1.name())
# for syn2 in s2_disambiguated:
#   print(syn2.name())
#print(wn.synset('pull_the_leg_of.v.01').lemma_names())
print(s1_tokenized)
print(s2_tokenized)
#M = wordnet_synset_similarity(s1_disambiguated,s2_disambiguated,s1_clean_tokenized, s2_clean_tokenized, M)
#print(M)

def similarity(vec1, vec2):
    #print (vec1, vec2)
    if np.linalg.norm(vec1) * np.linalg.norm(vec2) == 0:
      return 0
    return 1 - spatial.distance.cosine(vec1,vec2)

#This function takes two sentences in the form of word vectors as input
def cosineSimilarityMatrix(S_i, T_j):
  A=np.zeros((S_i.shape[1],T_j.shape[1]),dtype=np.float32) #d-dimensioonal vectors for each word by len of sentence. A dXn matrix for a sentence
  #print(A.shape)
  for i in range(S_i.shape[1]):
    v1= S_i[:,i]
    #print(v1.shape)
    for j in range(T_j.shape[1]):
      v2=T_j[:,j]
      #print(v2.shape)
      A[i,j]=similarity(v1,v2)
  return A

def embedWords(S):
  embedding = np.zeros((nlp(S[0]).vector.shape[0],len(S)),dtype=np.float32)
  for i, word in enumerate(S):
    embedding[:,i] = nlp(word).vector
  return embedding

def embedKannadaWord(S):
  embedding = np.zeros((nlp(S[0]).vector.shape[0],len(S)),dtype=np.float32)
  for i, word in enumerate(S):
    embedding[:,i] = ft.get_word_vector(word)
  return embedding

#embedded_Sent1 = embedWords(sent1_tokenized)
embedded_Sent1 = embedKannadaWord(sent1_tokenized)
embedded_Sent2 = embedWords(sent2_tokenized)
len1 = len(sent1_tokenized)
len2 = len(sent2_tokenized)

A = cosineSimilarityMatrix(embedded_Sent1,embedded_Sent2)




# import pandas as pd
# a = pd.DataFrame(np.transpose(A))
# print('--------Aij---------')
# print(a.to_string(na_rep='-'))
# b = pd.DataFrame(M)
# print('--------Mij---------')
# print(b.to_string(na_rep='-'))

# print(a.subtract(b.values))
# for i, w1 in enumerate(sent1_tokenized):
#   for j, w2 in enumerate(sent2_tokenized):
#     A[i,j]= max(A[i,j], M[w1][w2])

print(A)

# a = pd.DataFrame(np.transpose(A))
# b = pd.DataFrame(M)
# print(a.subtract(b.values))

#implementation of global form function for fmatch
def fmatch(i, T, A):
  sum_aij = np.sum(A[i,:])
  si_cap=np.zeros((T.shape[0],1))
  for j in range(T.shape[1]):
    si_cap=np.add(si_cap,A[i,j]*T[:,j].reshape(-1,1))
    #print((A[i,j]*T[:,j]).shape)
  si_cap=si_cap/sum_aij
  #print('Shape is ',si_cap.shape)
  return si_cap

fmatch(1,embedded_Sent2,A);

def Semantic_match(S,T, A):
  S_cap = np.zeros(S.shape, dtype=np.float32)
  T_cap = np.zeros(T.shape, dtype=np.float32)

  for i in range(S.shape[1]):
    si_cap = fmatch(i,T,A)
    #print(si_cap.shape)
    S_cap[:,i] = si_cap.reshape(-1)
  for j in range(T.shape[1]):
    tj_cap = fmatch(j,S,np.transpose(A))
    T_cap[:,j] = tj_cap.reshape(-1)
  return S_cap, T_cap

S_cap, T_cap=Semantic_match(embedded_Sent1,embedded_Sent2,A)

def orthogonal(si,si_cap):
  si_si_cap  =  np.dot(si,si_cap)
  si_cap_si_cap = np.dot(si_cap,si_cap)
  si_plus = (si_si_cap/si_cap_si_cap)*si_cap
  si_minus = si -si_plus
  return si_plus, si_minus

def decompose(S,S_cap):
  S_plus = np.zeros(S.shape,dtype=np.float32)
  S_minus = np.zeros(S.shape,dtype=np.float32)
  for i in range(S.shape[1]):
    si_plus, si_minus = orthogonal(S[:,i].reshape(-1),S_cap[:,i].reshape(-1))
    S_plus[:,i] = si_plus.reshape(-1)
    S_minus[:,i] = si_minus.reshape(-1)
  return S_plus, S_minus

def padSentence(A, seq_length=16):
  return np.pad(A, ((0,0),(0,seq_length-A.shape[1])), 'constant')  #Top bottom left right

seq_length=16
S_plus, S_minus = decompose(embedded_Sent1,S_cap)
T_plus, T_minus = decompose(embedded_Sent2,T_cap)
S_plus = np.transpose(padSentence(S_plus))
S_minus = np.transpose(padSentence(S_minus))
T_plus = np.transpose(padSentence(T_plus))
T_minus = np.transpose(padSentence(T_minus))

import tensorflow as tf

from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import tensorflow.compat.v1 as tf

#tf.disable_v2_behavior()

from keras.regularizers import l2
def get_model():
    inputs = tf.keras.Input(shape=(seq_length,300))
    convs = []
    filter_sizes = [2,3,4,5]
    for filter_size in filter_sizes:
      l_conv = tf.keras.layers.Conv1D(filters=128, kernel_size=filter_size, activation='relu',  kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activity_regularizer=l2(1e2))(inputs)
      l_pool = tf.keras.layers.GlobalMaxPooling1D()(l_conv)
      convs.append(l_pool)
    x1 = tf.keras.layers.concatenate(convs, axis=1)
    #x1 = tf.keras.layers.Conv1D(filters=128, kernel_size=2, activation='relu')(inputs)
    #outputs = tf.keras.layers.GlobalMaxPooling1D()(x1)
    #return tf.keras.Model(inputs, outputs)
    return tf.keras.Model(inputs, x1)

model1 = get_model()
model2 = get_model()
model3 = get_model()
model4 = get_model()

input1 = tf.keras.Input((seq_length, 300))
input2 = tf.keras.Input((seq_length, 300))
input3 = tf.keras.Input((seq_length, 300))
input4 = tf.keras.Input((seq_length, 300))

y1 = model1(input1)
y2 = model2(input2)
y3 = model3(input3)
y4 = model4(input4)
concat_layer = tf.keras.layers.concatenate([y1, y2, y3, y4],axis=1)
x =  tf.keras.layers.Dense(512, activation='relu')(concat_layer)
y = layers.Dense(1, activation='sigmoid')(x)
ensemble_model = tf.keras.Model(inputs=[input1, input2, input3, input4], outputs=y)


ensemble_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc']) #'binary_crossentropy'


ensemble_model.summary()
tf.keras.utils.plot_model(ensemble_model, "mini_resnet.png", show_shapes=True)

# s_plus_input=tf.placeholder("float", [None, seq_length,300])
# convs_s_plus = []
# filter_sizes = [3]

# for filter_size in filter_sizes:
#   l_conv_s_plus = tf.keras.layers.Conv1D(filters=128, kernel_size=filter_size, activation='relu')(s_plus_input)
#   l_pool_s_plus = tf.keras.layers.GlobalMaxPooling1D()(l_conv_s_plus)
#   convs_s_plus.append(l_pool_s_plus)
# l_merge_plus = tf.keras.layers.concatenate(convs_s_plus, axis=1)

# s_minus_input=tf.placeholder("float", [None, seq_length,300])
# convs_s_minus = []
# for filter_size in filter_sizes:
#   l_conv_s_minus = tf.keras.layers.Conv1D(filters=128, kernel_size=filter_size, activation='relu')(s_minus_input)
#   l_pool_s_minus = tf.keras.layers.GlobalMaxPooling1D()(l_conv_s_minus)
#   convs_s_minus.append(l_pool_s_minus)

# l_merge_minus = tf.keras.layers.concatenate(convs_s_minus, axis=1)


# t_plus_input=tf.placeholder("float", [None, seq_length,300])
# convs_t_plus = []
# for filter_size in filter_sizes:
#   l_conv_t_plus = tf.keras.layers.Conv1D(filters=128, kernel_size=filter_size, activation='relu')(t_plus_input)
#   l_pool_t_plus = tf.keras.layers.GlobalMaxPooling1D()(l_conv_t_plus)
#   convs_t_plus.append(l_pool_t_plus)

# t_merge_plus = tf.keras.layers.concatenate(convs_t_plus, axis=1)

# t_minus_input=tf.placeholder("float", [None, seq_length,300])
# convs_t_minus = []
# for filter_size in filter_sizes:
#   l_conv_t_minus = tf.keras.layers.Conv1D(filters=128, kernel_size=filter_size, activation='relu')(t_minus_input)
#   l_pool_t_minus = tf.keras.layers.GlobalMaxPooling1D()(l_conv_t_minus)
#   convs_t_minus.append(l_pool_t_minus)

# t_merge_minus = tf.keras.layers.concatenate(convs_t_minus, axis=1)

# merge_all = tf.keras.layers.concatenate([l_merge_plus, l_merge_minus, t_merge_plus, t_merge_minus], axis=1)


# x =  tf.keras.layers.Dense(128, activation='relu')(merge_all)
# preds =  tf.keras.layers.Dense(1,activation='softmax')(x)
# model =  tf.keras.Model(merge_all, preds)

# model.compile(loss='mean_squared_error',
#                   optimizer='adam',
#                   metrics=['acc'])

DATASET_PATH = "./drive/MyDrive/Colab Notebooks/SemEvalDataset/"#data/HAR_pose_activities/database/"
#filename = "msr_paraphrase_train.txt"

# from google.colab import drive
# drive.mount('/content/drive')
# import pandas as pd

# filename = "Preprocessed-Sentence-Kan-Eng-Training.xlsx"
# df=pd.read_excel(DATASET_PATH + filename)
# df=df.dropna()
# df_sentence_pair = df.loc[:, 'Indic Sentence':'English Sentence']
# df_score = df['Labse Score']
# sentences_A = df_sentence_pair['Indic Sentence'].values
# sentences_B = df_sentence_pair['English Sentence'].values
# score = df_score.values #> 3.4  #



def prepareData(train=True):
  #print(df.keys())
  filename = "Preprocessed-Sentence-Kan-Eng-Training.xlsx"
  if(train):
    df=pd.read_excel(DATASET_PATH + filename)
  else:
    filename = 'Preprocessed-Sentence-Kan-Eng-Testing.xlsx'
    df=pd.read_excel(DATASET_PATH + filename)
  df=df.dropna()
  df_sentence_pair = df.loc[:, 'Indic Sentence':'English Sentence']

  bucket = ["Marginally reject", "Marginally accept","Definitely accept"]
  df_score = [bucket.index(i) for i in df['bucket'].values]  #df['Labse Score']
  sentences_A = df_sentence_pair['Indic Sentence'].values
  sentences_B = df_sentence_pair['English Sentence'].values
  score = np.array(df_score) > 1 #df_score.values  #> 3.4  #
  X_data=[]
  y_label= []
  for i in range(len(sentences_A)):
    sent1 = sentences_A[i]
    sent2 = sentences_B[i]
    sent1_tokenized = getTokens(sent1)
    sent2_tokenized = getTokens(sent2)
    if(len(sent1_tokenized) > 15 or len(sent2_tokenized) > 15):
      continue
    embedded_Sent1 = embedKannadaWord(sent1_tokenized)#embedWords(sent1_tokenized)

    embedded_Sent2 = embedWords(sent2_tokenized)
    #print(sent1_tokenized, sent2_tokenized)
    A = cosineSimilarityMatrix(embedded_Sent1,embedded_Sent2)
    #Select the best measure
    S_cap, T_cap=Semantic_match(embedded_Sent1,embedded_Sent2,A)
    seq_length=16
    S_plus, S_minus = decompose(embedded_Sent1,S_cap)
    T_plus, T_minus = decompose(embedded_Sent2,T_cap)

    S_plus = np.transpose(padSentence(S_plus))
    S_minus = np.transpose(padSentence(S_minus))
    T_plus = np.transpose(padSentence(T_plus))
    T_minus = np.transpose(padSentence(T_minus))
    X_data.append([S_plus, S_minus, T_plus, T_minus])
    y_label.append(score[i])
  return X_data, y_label
# def prepareData(train=True):
#   #print(df.keys())
#   filename = "msr_paraphrase_train.txt"
#   if(train):
#     df=pd.read_csv(DATASET_PATH + filename, sep='\t', error_bad_lines=False)
#   else:
#     filename = 'msr_paraphrase_test.txt'
#     df=pd.read_csv(DATASET_PATH + filename, sep='\t', error_bad_lines=False)
#   df=df.dropna()
#   df_sentence_pair = df.loc[:, '#1 String':'#2 String']
#   df_score = df['Quality']
#   sentences_A = df_sentence_pair['#1 String'].values
#   sentences_B = df_sentence_pair['#2 String'].values
#   score = df_score.values #> 3.4  #
#   X_data=[]
#   y_label= []
#   for i in range(len(sentences_A)):
#     sent1 = sentences_A[i]
#     sent2 = sentences_B[i]
#     sent1_tokenized = getTokens(sent1)
#     sent2_tokenized = getTokens(sent2)
#     if(len(sent1_tokenized) > 15 or len(sent2_tokenized) > 15):
#       continue
#     embedded_Sent1 = embedWords(sent1_tokenized)
#     embedded_Sent2 = embedWords(sent2_tokenized)
#     #print(sent1_tokenized, sent2_tokenized)
#     A = cosineSimilarityMatrix(embedded_Sent1,embedded_Sent2)
#     #Select the best measure
#     S_cap, T_cap=Semantic_match(embedded_Sent1,embedded_Sent2,A)
#     seq_length=16
#     S_plus, S_minus = decompose(embedded_Sent1,S_cap)
#     T_plus, T_minus = decompose(embedded_Sent2,T_cap)

#     S_plus = np.transpose(padSentence(S_plus))
#     S_minus = np.transpose(padSentence(S_minus))
#     T_plus = np.transpose(padSentence(T_plus))
#     T_minus = np.transpose(padSentence(T_minus))
#     X_data.append([S_plus, S_minus, T_plus, T_minus])
#     y_label.append(score[i])
#   return X_data, y_label

# def prepareData(train=True):
#   df=pd.read_csv(DATASET_PATH + filename, sep='\t', error_bad_lines=False)
#   #print(df.keys())
#   if(train):
#     df=df[df["SemEval_set"] == "TRAIN"]
#   else:
#     df=df[df["SemEval_set"] == "TEST"]
#   df=df.dropna()
#   df_sentence_pair = df.loc[:, 'sentence_A':'sentence_B']
#   df_score = df['relatedness_score']
#   sentences_A = df_sentence_pair['sentence_A'].values
#   sentences_B = df_sentence_pair['sentence_B'].values
#   score = df_score.values > 3.4  #
#   X_data=[]
#   y_label= []
#   for i in range(len(sentences_A)):
#     sent1 = sentences_A[i]
#     sent2 = sentences_B[i]
#     sent1_tokenized = getTokens(sent1)
#     sent2_tokenized = getTokens(sent2)
#     if(len(sent1_tokenized) > 15 or len(sent2_tokenized) > 15):
#       continue
#     embedded_Sent1 = embedWords(sent1_tokenized)
#     embedded_Sent2 = embedWords(sent2_tokenized)
#     #print(sent1_tokenized, sent2_tokenized)
#     A = cosineSimilarityMatrix(embedded_Sent1,embedded_Sent2)
#     #Select the best measure
#     S_cap, T_cap=Semantic_match(embedded_Sent1,embedded_Sent2,A)
#     seq_length=16
#     S_plus, S_minus = decompose(embedded_Sent1,S_cap)
#     T_plus, T_minus = decompose(embedded_Sent2,T_cap)

#     S_plus = np.transpose(padSentence(S_plus))
#     S_minus = np.transpose(padSentence(S_minus))
#     T_plus = np.transpose(padSentence(T_plus))
#     T_minus = np.transpose(padSentence(T_minus))
#     X_data.append([S_plus, S_minus, T_plus, T_minus])
#     y_label.append(score[i])
#   return X_data, y_label

X_train_data, y_train_label = prepareData()
X_test_data, y_test_label = prepareData(train=False)

# #print(X_data)
# f=np.zeros(len1+1, len2+1)
# dist=np.zeros(len1+1, len2+1)
# f[0, 0]=0
# f[0,:]=np.Infinity
# f[:,0]=np.Infinity
# def wordOrder():
#   for i in range(0,len1+1):
#     for j in range(0,len2+1):
#       f[i,j] = di

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

!cd .config/

!ls

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./logs

import datetime, os
X_train_data = np.nan_to_num(np.array(X_train_data),nan=0)
y_train_label = np.array(y_train_label)
x_splus=X_train_data[:,0,:,:].reshape(-1, 16,300)
x_sminus=X_train_data[:,1,:,:].reshape(-1, 16,300)
x_tplus=X_train_data[:,2,:,:].reshape(-1, 16,300)
x_tminus=X_train_data[:,3,:,:].reshape(-1, 16,300)

################################################################
## Test data
#####################################################
#ynew = ensemble_model.predict(X_test_data)
X_test_data=np.nan_to_num(np.array(X_test_data),nan=0)
y_test_label=np.array(y_test_label)
x_splus_test=X_test_data[:,0,:,:].reshape(-1, 16,300)
x_sminus_test=X_test_data[:,1,:,:].reshape(-1, 16,300)
x_tplus_test=X_test_data[:,2,:,:].reshape(-1, 16,300)
x_tminus_test=X_test_data[:,3,:,:].reshape(-1, 16,300)

logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

history = ensemble_model.fit([x_splus,x_sminus,x_tplus,x_tminus], y_train_label, epochs=40, batch_size=1,  verbose=1, validation_data=([x_splus_test,x_sminus_test,x_tplus_test,x_tminus_test], y_test_label), callbacks=[tensorboard_callback])

scores = ensemble_model.evaluate([x_splus_test,x_sminus_test,x_tplus_test,x_tminus_test], y_test_label, verbose=1)
print("%s: %.2f%%" % ( ensemble_model.metrics_names[1], scores[1]*100))

print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

print(scores)

from tensorboard import notebook
notebook.list() # View open TensorBoard instances

